{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es una adaptación de un modelo modilarizado que se puede encontra documentado en: https://github.com/ManuelLagunas/Telecom.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries ----------------------------------------\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data ----------------------------------------\n",
    "\n",
    "contract_raw = pd.read_csv(\"/datasets/final_provider/contract.csv\")\n",
    "internet_raw = pd.read_csv(\"/datasets/final_provider/internet.csv\")\n",
    "personal_raw = pd.read_csv(\"/datasets/final_provider/personal.csv\")\n",
    "phone_raw = pd.read_csv(\"/datasets/final_provider/phone.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrección de errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct name columns ----------------------------------------\n",
    "\n",
    "def snake_case_columns(df):\n",
    "    df.columns = df.columns.map(lambda x: re.sub(r'(?<=[a-z])(?=[A-Z])', '_', x).lower())\n",
    "    return df\n",
    "\n",
    "contract_df = snake_case_columns(contract_raw)\n",
    "internet_df = snake_case_columns(internet_raw)\n",
    "personal_df = snake_case_columns(personal_raw)\n",
    "phone_df = snake_case_columns(phone_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct columns dtype ----------------------------------------\n",
    "\n",
    "#-------- contract_df --------\n",
    "contract_df['begin_date'] = pd.to_datetime(contract_df['begin_date'])\n",
    "contract_df['begin_date'] = (contract_df['begin_date'] - contract_df['begin_date'].min()).dt.days\n",
    "contract_df['end_date'] = contract_df['end_date'].apply(lambda x: 0 if x == 'No' else 1)\n",
    "contract_df = pd.get_dummies(contract_df, columns=['type'], prefix='type', drop_first=True)\n",
    "contract_df = pd.get_dummies(contract_df, columns=['paperless_billing'], prefix='paperless_billing', drop_first=True)\n",
    "contract_df = pd.get_dummies(contract_df, columns=['payment_method'], prefix='payment_method', drop_first=True)\n",
    "contract_df['total_charges'] = pd.to_numeric(contract_df['total_charges'], errors='coerce')\n",
    "# contract_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- internet_df --------\n",
    "cols_to_encode = [col for col in internet_df.columns if col not in ['customer_id']]\n",
    "internet_df = pd.get_dummies(internet_df, columns=cols_to_encode, drop_first=True)\n",
    "# internet_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- personal_df --------\n",
    "cols_to_encode = [col for col in personal_df.columns if col not in ['customer_id', 'senior_citizen']]\n",
    "personal_df = pd.get_dummies(personal_df, columns=cols_to_encode, drop_first=True)\n",
    "personal_df['senior_citizen'] = personal_df['senior_citizen'].astype('bool')\n",
    "# personal_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- phone_df --------\n",
    "phone_df = pd.get_dummies(phone_df, columns=['multiple_lines'], prefix='multiple_lines', drop_first=True)\n",
    "# phone_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fusión de dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframes fusion ----------------------------------------\n",
    "\n",
    "merged_df = pd.merge(contract_df, internet_df, on='customer_id', how='outer')\n",
    "merged_df = pd.merge(merged_df, personal_df, on='customer_id', how='outer')\n",
    "merged_df = pd.merge(merged_df, phone_df, on='customer_id', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values ----------------------------------------\n",
    "\n",
    "merged_df['total_charges'].dropna(inplace=True)\n",
    "merged_df.fillna(False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct columns dtype ----------------------------------------\n",
    "\n",
    "# merged_df['begin_date'] = pd.to_datetime(merged_df['begin_date'])\n",
    "merged_df['end_date'] = pd.to_numeric(merged_df['end_date'], errors='coerce')\n",
    "merged_df['total_charges'] = pd.to_numeric(merged_df['total_charges'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estudio de desbalanceo de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unbalanced data ----------------------------------------\n",
    "\n",
    "# Count the number of 1s and 0s in the 'end_date' column\n",
    "count_1 = merged_df['end_date'].eq(1).sum()\n",
    "count_0 = merged_df['end_date'].eq(0).sum()\n",
    "\n",
    "# Print the counts\n",
    "print(\"Number of 1s:\", count_1)\n",
    "print(\"Number of 0s:\", count_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting data ----------------------------------------\n",
    "sns.countplot(data=merged_df, x='end_date')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of 1s and 0s in the 'end_date' column\n",
    "percentage_1 = count_1 / len(merged_df) * 100\n",
    "percentage_0 = count_0 / len(merged_df) * 100\n",
    "\n",
    "# Print the percentages\n",
    "print(\"Percentage of 1s:\", percentage_1)\n",
    "print(\"Percentage of 0s:\", percentage_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the ratio of 0s to 1s in the 'end_date' column\n",
    "ratio_0_to_1 = count_0 / count_1\n",
    "# Print the ratio\n",
    "print(\"Ratio of 0s to 1s:\", ratio_0_to_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de características y objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features creation ----------------------------------------\n",
    "\n",
    "# df.columns\n",
    "features = merged_df.drop(columns=['customer_id', 'end_date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target creation ----------------------------------------\n",
    "\n",
    "target = merged_df['end_date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data ----------------------------------------\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sobremuestreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsampler creation ---------------------------------------- \n",
    "\n",
    "def upsample(features, target):\n",
    "    # Convert target to a pandas Series if it's a one-column DataFrame\n",
    "    if isinstance(target, pd.DataFrame):\n",
    "        target = target.squeeze()\n",
    "\n",
    "    # Combine features and target\n",
    "    df = pd.concat([features, target], axis=1)\n",
    "\n",
    "    # Class separation\n",
    "    df_majority = df[target==0]\n",
    "    df_minority = df[target==1]\n",
    "\n",
    "    # Upsampling the minority class\n",
    "    df_minority_upsampled = resample(df_minority, \n",
    "                                     replace=True,     \n",
    "                                     n_samples=len(df_majority),    \n",
    "                                     random_state=123) \n",
    "\n",
    "    # Combine the majority class with the minority upsampled class\n",
    "    df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "    # Shuffle the data\n",
    "    df_upsampled = df_upsampled.sample(frac=1, random_state=123)\n",
    "\n",
    "    # Separate features and target\n",
    "    features_upsampled = df_upsampled.drop(target.name, axis=1)\n",
    "    target_upsampled = df_upsampled[target.name]\n",
    "\n",
    "    return features_upsampled, target_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsampling ----------------------------------------\n",
    "\n",
    "features_train_upsampled, target_train_upsampled = upsample(features_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submuestreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsampler creation ----------------------------------------\n",
    "\n",
    "def downsample(features, target):\n",
    "    # Convert target to a pandas Series if it's a one-column DataFrame\n",
    "    if isinstance(target, pd.DataFrame):\n",
    "        target = target.squeeze()\n",
    "\n",
    "    # Combine features and target\n",
    "    df = pd.concat([features, target], axis=1)\n",
    "\n",
    "    # Class separation\n",
    "    df_majority = df[target==0]\n",
    "    df_minority = df[target==1]\n",
    "\n",
    "    # Calculate the fraction of majority samples to keep\n",
    "    fraction = len(df_minority) / len(df_majority)\n",
    "\n",
    "    # Downsample the majority class\n",
    "    df_majority_downsampled = df_majority.sample(frac=fraction, random_state=123)\n",
    "\n",
    "    # Combine the minority class with the downsampled majority class\n",
    "    df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "    # Shuffle the data\n",
    "    df_downsampled = df_downsampled.sample(frac=1, random_state=123)\n",
    "\n",
    "    # Separate features and target\n",
    "    features_downsampled = df_downsampled.drop(target.name, axis=1)\n",
    "    target_downsampled = df_downsampled[target.name]\n",
    "\n",
    "    return features_downsampled, target_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsampling ----------------------------------------\n",
    "\n",
    "features_train_downsampled, target_train_downsampled = downsample(features_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning models ----------------------------------------\n",
    "\n",
    "# ---------------- Logistic Regression ----------------\n",
    "# Define the hyperparameters grid\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "# Create the logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=logreg, param_grid=param_grid, scoring='roc_auc', cv=5)\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(features_train, target_train)\n",
    "\n",
    "# Get the best hyperparameters and the corresponding AUC-ROC score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Print the best hyperparameters and the corresponding AUC-ROC score\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best AUC-ROC Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Random Forest ----------------\n",
    "# Define the hyperparameters grid\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 20, 30],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "# Create the random forest model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Create scorer\n",
    "auc_roc_scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf, param_grid=param_grid, scoring=auc_roc_scorer, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(features_train, target_train)\n",
    "\n",
    "# Get the best hyperparameters and the corresponding AUC-ROC score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Print the best hyperparameters and the corresponding AUC-ROC score\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best AUC-ROC Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- LightGBM ----------------\n",
    "# Define the hyperparameters grid\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 20, 30],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'num_leaves': [31, 62, 93],\n",
    "    'min_child_samples': [20, 30, 40]\n",
    "}\n",
    "\n",
    "# Create the LightGBM model\n",
    "lgbm = LGBMClassifier()\n",
    "\n",
    "# Create scorer\n",
    "auc_roc_scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=lgbm, param_grid=param_grid, scoring=auc_roc_scorer, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(features_train, target_train)\n",
    "\n",
    "# Get the best hyperparameters and the corresponding AUC-ROC score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Print the best hyperparameters and the corresponding AUC-ROC score\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best AUC-ROC Score:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos con balanceo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning models ----------------------------------------\n",
    "\n",
    "# ---------------- Logistic Regression ----------------\n",
    "# Define the hyperparameters grid\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "# Create the logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=logreg, param_grid=param_grid, scoring='roc_auc', cv=5)\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(features_train, target_train)\n",
    "\n",
    "# Get the best hyperparameters and the corresponding AUC-ROC score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Print the best hyperparameters and the corresponding AUC-ROC score\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best AUC-ROC Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Random Forest ----------------\n",
    "# Define the hyperparameters grid\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 20, 30],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'class_weight': ['balanced', 'balanced_subsample']\n",
    "}\n",
    "\n",
    "# Create the random forest model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Create scorer\n",
    "auc_roc_scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf, param_grid=param_grid, scoring=auc_roc_scorer, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(features_train, target_train)\n",
    "\n",
    "# Get the best hyperparameters and the corresponding AUC-ROC score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Print the best hyperparameters and the corresponding AUC-ROC score\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best AUC-ROC Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- LightGBM ----------------\n",
    "# Define the hyperparameters grid\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 20, 30],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'num_leaves': [31, 62, 93],\n",
    "    'min_child_samples': [20, 30, 40],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "# Create the LightGBM model\n",
    "lgbm = LGBMClassifier()\n",
    "\n",
    "# Create scorer\n",
    "auc_roc_scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=lgbm, param_grid=param_grid, scoring=auc_roc_scorer, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(features_train, target_train)\n",
    "\n",
    "# Get the best hyperparameters and the corresponding AUC-ROC score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Print the best hyperparameters and the corresponding AUC-ROC score\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best AUC-ROC Score:\", best_score)\n",
    "\n",
    "# Create the best model ----------------------------------------\n",
    "\n",
    "lgbm_balanced = LGBMClassifier(**best_params)\n",
    "lgbm_balanced.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos con sobremuestreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning models ----------------------------------------\n",
    "\n",
    "# ---------------- Logistic Regression ----------------\n",
    "# Define the hyperparameters grid\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "# Create the logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=logreg, param_grid=param_grid, scoring='roc_auc', cv=5)\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(features_train_upsampled, target_train_upsampled)\n",
    "\n",
    "# Get the best hyperparameters and the corresponding AUC-ROC score\n",
    "best_params_lr = grid_search.best_params_\n",
    "best_score_lr = grid_search.best_score_\n",
    "\n",
    "# Print the best hyperparameters and the corresponding AUC-ROC score\n",
    "print(\"Best Hyperparameters:\", best_params_lr)\n",
    "print(\"Best AUC-ROC Score:\", best_score_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Random Forest ----------------\n",
    "# Define the hyperparameters grid\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 20, 30],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "# Create the random forest model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Create scorer\n",
    "auc_roc_scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf, param_grid=param_grid, scoring=auc_roc_scorer, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(features_train_upsampled, target_train_upsampled)\n",
    "\n",
    "# Get the best hyperparameters and the corresponding AUC-ROC score\n",
    "best_params_rf = grid_search.best_params_\n",
    "best_score_rf = grid_search.best_score_\n",
    "\n",
    "# Print the best hyperparameters and the corresponding AUC-ROC score\n",
    "print(\"Best Hyperparameters:\", best_params_rf)\n",
    "print(\"Best AUC-ROC Score:\", best_score_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LightGBM model\n",
    "lgbm = LGBMClassifier()\n",
    "\n",
    "# Create scorer\n",
    "auc_roc_scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=lgbm, param_grid=param_grid, scoring=auc_roc_scorer, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(features_train_upsampled, target_train_upsampled)\n",
    "\n",
    "# Get the best hyperparameters and the corresponding AUC-ROC score\n",
    "best_params_lgbm = grid_search.best_params_\n",
    "best_score_lgbm = grid_search.best_score_\n",
    "\n",
    "# Print the best hyperparameters and the corresponding AUC-ROC score\n",
    "print(\"Best Hyperparameters:\", best_params_lgbm)\n",
    "print(\"Best AUC-ROC Score:\", best_score_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the best model ----------------------------------------\n",
    "\n",
    "lgbm_upsampled = LGBMClassifier(**best_params_lgbm)\n",
    "lgbm_upsampled.fit(features_train_upsampled, target_train_upsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos con submuestreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Logistic Regression ----------------\n",
    "# Define the hyperparameters grid\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "# Create the logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=logreg, param_grid=param_grid, scoring='roc_auc', cv=5)\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(features_train_downsampled, target_train_downsampled)\n",
    "\n",
    "# Get the best hyperparameters and the corresponding AUC-ROC score\n",
    "best_params_lr = grid_search.best_params_\n",
    "best_score_lr = grid_search.best_score_\n",
    "\n",
    "# Print the best hyperparameters and the corresponding AUC-ROC score\n",
    "print(\"Best Hyperparameters:\", best_params_lr)\n",
    "print(\"Best AUC-ROC Score:\", best_score_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Random Forest ----------------\n",
    "# Define the hyperparameters grid\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 20, 30],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "# Create the random forest model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Create scorer\n",
    "auc_roc_scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf, param_grid=param_grid, scoring=auc_roc_scorer, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(features_train_downsampled, target_train_downsampled)\n",
    "\n",
    "# Get the best hyperparameters and the corresponding AUC-ROC score\n",
    "best_params_rf = grid_search.best_params_\n",
    "best_score_rf = grid_search.best_score_\n",
    "\n",
    "# Print the best hyperparameters and the corresponding AUC-ROC score\n",
    "print(\"Best Hyperparameters:\", best_params_rf)\n",
    "print(\"Best AUC-ROC Score:\", best_score_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LightGBM model\n",
    "lgbm = LGBMClassifier()\n",
    "\n",
    "# Create scorer\n",
    "auc_roc_scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=lgbm, param_grid=param_grid, scoring=auc_roc_scorer, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(features_train_downsampled, target_train_downsampled)\n",
    "\n",
    "# Get the best hyperparameters and the corresponding AUC-ROC score\n",
    "best_params_lgbm = grid_search.best_params_\n",
    "best_score_lgbm = grid_search.best_score_\n",
    "\n",
    "# Print the best hyperparameters and the corresponding AUC-ROC score\n",
    "print(\"Best Hyperparameters:\", best_params_lgbm)\n",
    "print(\"Best AUC-ROC Score:\", best_score_lgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model application ----------------------------------------\n",
    "\n",
    "predictions = lgbm_upsampled.predict(features_test)\n",
    "auc_roc = roc_auc_score(target_test, predictions)\n",
    "print(\"AUC-ROC:\", auc_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC-ROC graph ----------------------------------------\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(target_test, predictions)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
